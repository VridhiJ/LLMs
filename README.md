# Understanding LLMs
## Transformer-Architecture: 
Implementing Attention is all you need transformer architecture from scratch. 

## Finetuning LLMs:
Finetuning LLMs using LoRA and IA3 implemented through various papers.

## Memory optimization - Quantization
Quantization Aware Training and post-training quantization
