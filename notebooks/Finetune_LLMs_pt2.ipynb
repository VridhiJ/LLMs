{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color:green;font-size: 3em;\">\n",
    "Implementing Fine-tuning Techniques</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing various fine-tuning methods as described in different papers, specifically LoRA and IA3.\n",
    "\n",
    "Pt2:\n",
    "\n",
    "In this notebook, we will:\n",
    "- Inject LoRA Adapters into our model\n",
    "- Finetune our LoRA Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, T5Tokenizer, T5ForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Adapters\n",
    "\n",
    "In this section, we will implement LoRA (Low-Rank Adaptation) and inject it into our causal model. Specifically, we will inject LoRA into the **key, query, and value** matrices of each transformer block.\n",
    "\n",
    "Recall from the LoRA paper that LoRA enhances model training efficiency by reducing the need to retrain all pretrained weights. Instead, it introduces two smaller matrices, A and B, which capture the necessary adaptations for the new task. This significantly reduces computational overhead while maintaining high performance.\n",
    "\n",
    "For more information, read the [paper](https://arxiv.org/pdf/2106.09685).\n",
    "\n",
    "By using LoRA in our causal model, we aim to achieve efficient fine-tuning with minimal computational cost, focusing on the key, query, and value matrices within each transformer block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA class\n",
    "\n",
    "First, let's implement the LoRA class based on how it is defined in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        r: int,\n",
    "        lora_alpha: int,\n",
    "        lora_dropout: float,\n",
    "    ):\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        # Optional dropout\n",
    "        if lora_dropout > 0.:\n",
    "            self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "        else:\n",
    "            self.lora_dropout = lambda x: x\n",
    "\n",
    "class LoRAAdapter(nn.Module, LoRALayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        existing_layer: nn.Module,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        lora_dropout: float = 0.,\n",
    "        **kwargs\n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout)\n",
    "        self.existing_layer = existing_layer\n",
    "\n",
    "\n",
    "        self.r = r # Rank of LoRA Adapter\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(torch.randn(r, in_features))\n",
    "            self.lora_B = nn.Parameter(torch.zeros(out_features, r))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    ## Resets the two matrices (A and B) based on how the paper does it\n",
    "    def reset_parameters(self):\n",
    "        if self.r > 0:\n",
    "            nn.init.normal_(self.lora_A, mean=0.0, std=1.0)\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        self.existing_layer.train(mode)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "      if self.r > 0:\n",
    "        # change x shape for matrice multiplication\n",
    "        batch_size, seq_len, in_features = x.shape\n",
    "        x = x.view(-1, in_features)  # Shape: (batch_size * seq_len, in_features)\n",
    "\n",
    "        # Ensure dtype consistency\n",
    "        x = x.to(torch.bfloat16)\n",
    "\n",
    "        # LoRA output: B(A(x)) * scaling\n",
    "        lora_out = torch.matmul(self.lora_A, x.T)  # Shape: (r, batch_size * seq_len)\n",
    "        lora_out = torch.matmul(self.lora_B, lora_out)  # Shape: (out_features, batch_size * seq_len)\n",
    "        lora_out = lora_out.T * self.scaling  # Shape: (batch_size * seq_len, out_features)\n",
    "\n",
    "        # dropout\n",
    "        lora_out = self.lora_dropout(lora_out)\n",
    "\n",
    "        # Add lora_out to the existing layer's output\n",
    "        return self.existing_layer(x) + lora_out\n",
    "      else:\n",
    "        # If r is zero, return the existing layer's output\n",
    "        return self.existing_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inject into the model\n",
    "\n",
    "Recall in LoRA that we want to freeze the pre-trained model and only train our adapter weights `lora_A` and `lora_B`.  \n",
    "\n",
    "\n",
    "Here we will use method: `mask_only_lora_as_trainable` so that only those weights require gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_only_lora_as_trainable(model: nn.Module) -> None:\n",
    "    # Freeze all parameters in the model\n",
    "    for param in model.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "    # Enable gradients only for LoRA parameters\n",
    "    for name, param in model.named_parameters():\n",
    "      if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to write the code that will inject the LoRA adapters into our causal model.\n",
    "\n",
    "\n",
    "`match_submodules`: Returns a list of names of layers in a model whose names match a specified key.\n",
    "\n",
    "`get_submodule`: Retrieves a specific submodule from a model based on its name.\n",
    "\n",
    "`replace_submodule`: Replaces a specific submodule in a model with a new module at a given path.\n",
    "\n",
    "\n",
    "`inject_adapter`: Replaces all submodules in a model that match any string in a list with a new module created by an adapter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_submodules(model: nn.Module, key:str) -> List[str]:\n",
    "  matching_layers = []\n",
    "  for name, module in model.named_modules():\n",
    "    if key in name:\n",
    "      matching_layers.append(name)\n",
    "  return matching_layers\n",
    "\n",
    "def get_submodule(model: nn.Module, module_name:str):\n",
    "    return model.get_submodule(module_name)\n",
    "\n",
    "def replace_submodule(model: nn.Module, module_path: str, new_module):\n",
    "  modules = module_path.split('.')\n",
    "  parent_module = model\n",
    "  for sub in modules[:-1]:\n",
    "    parent_module = getattr(parent_module, sub)\n",
    "  setattr(parent_module, modules[-1], new_module)\n",
    "\n",
    "def inject_adapter(model: nn.Module, match_on: List[str], adapter_fn):\n",
    "  for key in match_on:\n",
    "    matching_layers = match_submodules(model, key)\n",
    "    for module_path in matching_layers:\n",
    "      current_module = get_submodule(model, module_path)\n",
    "      new_module = adapter_fn(current_module) # New LoRA module\n",
    "      new_module = new_module.to(current_module.weight.device) # Move to gpu\n",
    "      replace_submodule(model, module_path, new_module) # Replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on a benchmark\n",
    "\n",
    "Next, we want to inject the LoRA adapter into our causal model we defined earlier. Let's also check to see how many parameters are in this model, as well as how many of these parameters are considered trainable.\n",
    "\n",
    "\n",
    "Re-initialize the causal model and check the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-initialize the causal model\n",
    "causal_model_name = \"facebook/opt-125m\"\n",
    "causal_model = AutoModelForCausalLM.from_pretrained(causal_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "causal_tokenizer = AutoTokenizer.from_pretrained(causal_model_name)\n",
    "\n",
    "# Check the model architecture\n",
    "causal_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
