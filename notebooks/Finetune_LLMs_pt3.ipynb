{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color:green;font-size: 3em;\">\n",
    "Implementing Fine-tuning Techniques</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing various fine-tuning methods as described in different papers, specifically LoRA and IA3.\n",
    "\n",
    "In this notebook, we will explore IA3 implementations:\n",
    "- by modifying activations\n",
    "- by manipulating weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# importing required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, T5Tokenizer, T5ForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IA3 Memory Analysis\n",
    "\n",
    "In this section, we will perform memory analysis on a well-known model from Hugging Face called Gemma. Gemma is a state-of-the-art model used for various natural language processing tasks.\n",
    "\n",
    "However, it is a \"gated model\", so will require access.\n",
    "\n",
    "To read more about how to get access to gated models: [link](https://huggingface.co/docs/hub/en/models-gated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb54df0015e46959516b5bdaa804ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f072c994a43e4959b9a372bd1edf0395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4207d0c58a447f8f340083ac889e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a056063ffe4ee4ae52e147b87cb8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830c69178c1f457f9f0ef263523ca2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d209b1456f4340d285f269889c7de0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8652a60f7486473f89eba699c682ece2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e9aee29f1c47ba826e7e6334626439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61400029846f491c908f44f89e6313e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74982709f8c749d39da2be8efb1ee962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bb16eb85dc4e5782245888216cf7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Load the gemma model (NEED AUTHENTICATIOIN)\n",
    "model_name = \"google/gemma-2b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement IA3 with activations\n",
    "\n",
    "Next, similar to how we integrate LoRA adapters into our OPT model, we aim to incorporate IA3 into the Gemma model.\n",
    "\n",
    "More about IA3 [here](https://arxiv.org/pdf/2205.05638).\n",
    "\n",
    "IA3 has two distinct implementations: one that modifies activations (as traditional PEFT frameworks do) and another that adjusts weights and biases to align with the CLAM framework.\n",
    "\n",
    "Modifying activations means that during the forward pass, the outputs (also known as activations) of certain layers are adjusted using additional parameters.\n",
    "\n",
    "Manipulating weights and biases, on the other hand, means changing the way we get the weights such that it will still satisfy the fine-tuning technique. Below is an implementation focusing on manipulating activations. Review the code carefully to understand the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IA3Adapter1(nn.Module):\n",
    "    def __init__(self, existing_layer, in_features, out_features, ia3_lr, is_feedforward=False):\n",
    "        nn.Module.__init__(self)\n",
    "        self.existing_layer = existing_layer.to(\"cuda\")\n",
    "        self.is_feedforward = is_feedforward\n",
    "\n",
    "        # The trainable weights\n",
    "        self.ia3_lw = (\n",
    "            nn.Parameter(\n",
    "                torch.ones((1, out_features), device=\"cuda\")\n",
    "            )\n",
    "            if not is_feedforward\n",
    "            else nn.Parameter(\n",
    "                torch.ones((1, in_features), device=\"cuda\")\n",
    "            )\n",
    "        )\n",
    "        nn.init.ones_(self.ia3_lw)\n",
    "\n",
    "        self.ia3_lr = ia3_lr\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not self.is_feedforward:\n",
    "            # We first get the output of the current layer\n",
    "            result = self.existing_layer(x)\n",
    "            result = torch.mul(result, self.ia3_lw)\n",
    "            return result\n",
    "        else:\n",
    "            result = torch.mul(x, self.ia3_lw)\n",
    "            result = self.existing_layer(result)\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ia3_params(model: nn.Module) -> None:\n",
    "  total_params = 0\n",
    "  trainable_params = 0\n",
    "  # Freeze all parameters in the model\n",
    "  for param in model.parameters():\n",
    "    total_params += param.numel()\n",
    "\n",
    "  # Enable gradients only for ia3 parameters\n",
    "  for name, param in model.named_parameters():\n",
    "    if \"ia3_lw\" in name:\n",
    "      trainable_params += param.numel()\n",
    "  return total_params, trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_submodules(model: nn.Module, key:str) -> List[str]:\n",
    "  matching_layers = []\n",
    "  for name, module in model.named_modules():\n",
    "    if key in name:\n",
    "      matching_layers.append(name)\n",
    "  return matching_layers\n",
    "\n",
    "\n",
    "def get_submodule(model: nn.Module, module_name:str):\n",
    "    return model.get_submodule(module_name)\n",
    "\n",
    "\n",
    "def replace_submodule(model: nn.Module, module_path: str, new_module):\n",
    "  modules = module_path.split('.')\n",
    "  parent_module = model\n",
    "  for sub in modules[:-1]:\n",
    "    parent_module = getattr(parent_module, sub)\n",
    "  setattr(parent_module, modules[-1], new_module)\n",
    "\n",
    "\n",
    "def inject_adapter(model: nn.Module, match_on: List[str], adapter_fn):\n",
    "  processed_modules = set()\n",
    "  for key in match_on:\n",
    "    matching_layers = match_submodules(model, key)\n",
    "    for module_path in matching_layers:\n",
    "      if module_path in processed_modules:\n",
    "        continue\n",
    "      current_module = get_submodule(model, module_path)\n",
    "      new_module = adapter_fn(current_module) # New IA3 module\n",
    "      new_module = new_module.to(current_module.weight.device)  # Move to gpu\n",
    "      replace_submodule(model, module_path, new_module) # Replace\n",
    "      processed_modules.add(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inject_adapter(model, [\"k_proj\",\"v_proj\",\"down_proj\"], lambda x: IA3Adapter1(x, in_features=x.in_features, out_features=x.out_features, ia3_lr=1e-3, is_feedforward = [\"False\", \"False\", \"True\"]))\n",
    "total_params, trainable_params = ia3_params(model)\n",
    "\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Trainable Parameters: {trainable_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
