{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color:green;font-size: 3em;\">\n",
    "Implementing Fine-tuning Techniques</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing various fine-tuning methods as described in different papers, specifically LoRA and IA3.\n",
    "\n",
    "In this notebook, we will explore IA3 implementations:\n",
    "- by modifying activations\n",
    "- by manipulating weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# importing required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, T5Tokenizer, T5ForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IA3 Memory Analysis\n",
    "\n",
    "In this section, we will perform memory analysis on a well-known model from Hugging Face called Gemma. Gemma is a state-of-the-art model used for various natural language processing tasks.\n",
    "\n",
    "However, it is a \"gated model\", so will require access.\n",
    "\n",
    "To read more about how to get access to gated models: [link](https://huggingface.co/docs/hub/en/models-gated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986380f7bdde448183c0a9a9e6b43456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d0b57cd8e448b7a9675d0a7997a47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba53b7af55c4be78d54067350c4b726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1573b4b6b7694125a6e6e9ef8281e5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650535f7713d40439e3f062b88fccae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90452eaa6db0472c8ac6bd645781b048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9433a6a11e2a4dcab7dac1e7ada40063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202489582d9c4523bc2e5e212c1eea10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f06ae4034184ab7870378b54ea67e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5105e50065134d9b9f8b65ce0335e930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef62ed1f4d94ebaa619fb796ad64664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Load the gemma model (NEED AUTHENTICATIOIN)\n",
    "model_name = \"google/gemma-2b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement IA3 with activations\n",
    "\n",
    "Next, similar to how we integrate LoRA adapters into our OPT model, we aim to incorporate IA3 into the Gemma model.\n",
    "\n",
    "More about IA3 [here](https://arxiv.org/pdf/2205.05638).\n",
    "\n",
    "IA3 has two distinct implementations: one that modifies activations (as traditional PEFT frameworks do) and another that adjusts weights and biases to align with the CLAM framework.\n",
    "\n",
    "Modifying activations means that during the forward pass, the outputs (also known as activations) of certain layers are adjusted using additional parameters.\n",
    "\n",
    "Manipulating weights and biases, on the other hand, means changing the way we get the weights such that it will still satisfy the fine-tuning technique. Below is an implementation focusing on manipulating activations. Review the code carefully to understand the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IA3Adapter1(nn.Module):\n",
    "    def __init__(self, existing_layer, in_features, out_features, ia3_lr, is_feedforward=False):\n",
    "        nn.Module.__init__(self)\n",
    "        self.existing_layer = existing_layer.to(\"cuda\")\n",
    "        self.is_feedforward = is_feedforward\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # The trainable weights\n",
    "        self.ia3_lw = (\n",
    "            nn.Parameter(\n",
    "                torch.ones((1, out_features), device=\"cuda\")\n",
    "            )\n",
    "            if not is_feedforward\n",
    "            else nn.Parameter(\n",
    "                torch.ones((1, in_features), device=\"cuda\")\n",
    "            )\n",
    "        )\n",
    "        nn.init.ones_(self.ia3_lw)\n",
    "\n",
    "        self.ia3_lr = ia3_lr\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not self.is_feedforward:\n",
    "            # We first get the output of the current layer\n",
    "            result = self.existing_layer(x)\n",
    "            result = torch.mul(result, self.ia3_lw)\n",
    "            return result\n",
    "        else:\n",
    "            result = torch.mul(x, self.ia3_lw)\n",
    "            result = self.existing_layer(result)\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ia3_params(model: nn.Module) -> None:\n",
    "  total_params = 0\n",
    "  trainable_params = 0\n",
    "  # Freeze all parameters in the model\n",
    "  for param in model.parameters():\n",
    "    total_params += param.numel()\n",
    "\n",
    "  # Enable gradients only for ia3 parameters\n",
    "  for name, param in model.named_parameters():\n",
    "    if \"ia3_lw\" in name:\n",
    "      trainable_params += param.numel()\n",
    "  return total_params, trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_submodules(model: nn.Module, key:str) -> List[str]:\n",
    "  matching_layers = []\n",
    "  for name, module in model.named_modules():\n",
    "    if key in name:\n",
    "      matching_layers.append(name)\n",
    "  return matching_layers\n",
    "\n",
    "\n",
    "def get_submodule(model: nn.Module, module_name:str):\n",
    "    return model.get_submodule(module_name)\n",
    "\n",
    "\n",
    "def replace_submodule(model: nn.Module, module_path: str, new_module):\n",
    "  modules = module_path.split('.')\n",
    "  parent_module = model\n",
    "  for sub in modules[:-1]:\n",
    "    parent_module = getattr(parent_module, sub)\n",
    "  setattr(parent_module, modules[-1], new_module)\n",
    "\n",
    "\n",
    "def inject_adapter(model: nn.Module, match_on: List[str], adapter_fn):\n",
    "  processed_modules = set()\n",
    "  for key in match_on:\n",
    "    matching_layers = match_submodules(model, key)\n",
    "    for module_path in matching_layers:\n",
    "      if module_path in processed_modules:\n",
    "        continue\n",
    "      current_module = get_submodule(model, module_path)\n",
    "      new_module = adapter_fn(current_module) # New IA3 module\n",
    "      new_module = new_module.to(device)  # Move to gpu\n",
    "      replace_submodule(model, module_path, new_module) # Replace\n",
    "      processed_modules.add(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 2506541056\n",
      "Trainable Parameters: 368640\n"
     ]
    }
   ],
   "source": [
    "inject_adapter(model, [\"k_proj\",\"v_proj\",\"down_proj\"], lambda x: IA3Adapter1(x, in_features=x.in_features, out_features=x.out_features, ia3_lr=1e-3, is_feedforward = [\"False\", \"False\", \"True\"]))\n",
    "total_params, trainable_params = ia3_params(model)\n",
    "\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Trainable Parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): IA3Adapter1(\n",
       "            (existing_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          )\n",
       "          (v_proj): IA3Adapter1(\n",
       "            (existing_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          )\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): IA3Adapter1(\n",
       "            (existing_layer): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          )\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check architecture after IA3 interjection\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory analysis\n",
    "\n",
    "\n",
    "With the activations method, we will now calculate the memory requirement (return result in GB) of each forward pass when we try to inject IA3 into the **key and value matrices** of **each transformer block** in gemma and the **down_proj** layer of the mlp layer.\n",
    "\n",
    "\n",
    "First calculate how many GB of memory per block (separate KV calculations from down_proj calculations)\n",
    "\n",
    "The parameter weights of gemma is stored in Brain Float 16 which is 2 bytes (or 16 bits). Gradients are stored in 4 bytes (or 32 bits). We can disregard the calculations related to the optimizer. We also need to take care of the calculations for the additional trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal IA3 (IA3 Adapter 1)\n",
    "\n",
    "`For K,V:`\n",
    "\n",
    "x (batched): `[32,216,2048]` (batch size, sequence length, hidden dim) (not counted towards memory)\n",
    "\n",
    "self.existing_layer(x)’s shape: `[32,216,256]`\n",
    "\n",
    "torch.mul(result, self.ia3_lw) result’s shape: `[32,216,256]`\n",
    "\n",
    "`Total` = 2 matrices KV * 2 computations * 32 (first value of the shape of the previous operation)* 216(second value of the shape of the previous operation) * 256 (third value of the shape of the previous operation) * 16 bits per number / 8 bits per byte = `14,155,776 bytes`\n",
    "\n",
    "\n",
    "`For down_proj:`\n",
    "x (batched): `[32,216,16384]` (not counted towards memory)\n",
    "\n",
    "torch.mul(x, self.ia3_lw): `[32, 216, 16384]`\n",
    "\n",
    "self.existing_layer(result): `[32, 216, 2048]`\n",
    "\n",
    "`Total` = (32 * 216 * 16384 + 32 * 216 * 2048)  * 16 bits per number / 8 bits per byte =  `254,803,968 bytes`\n",
    "\n",
    "\n",
    "`Total activations size per block` = 14,155,776 bytes + 254,803,968 bytes = 268959744 bytes = `0.25 GB`\n",
    "\n",
    "\n",
    "`Total IA3 activation size for model`  = 0.25 GB * 18 blocks = `4.5087890625 GB`\n",
    "\n",
    "We also introduce more trainable parameters, whose weight values, gradients, and gradient moments need to be tracked for the optimizer.  Assume we are using the Adam optimizer, which requires 2 gradient moments to be stored per trainable parameter.\n",
    "\n",
    "Total trainable parameters: 368640\n",
    "\n",
    "Param weights (bf16): ) 368640 params * 2 bytes = 737280 bytes\n",
    "\n",
    "Gradients (32 bit) = 368640 params * 4 bytes = 1474560 bytes\n",
    "\n",
    "Gradient moments (32 bit) = n params * 2 moments * 4 bytes = 2949120 bytes\n",
    "\n",
    "`Total additional memory for params: 0.00480651855 GB`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement IA3 in CLAM\n",
    "\n",
    "In this section, the implementation modifies the weights and biases directly instead of manipulating activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part of the CLAM abstraction (should not compile)\n",
    "class IA3Adapter2(nn.Module):\n",
    "    def __init__(self, existing_layer, in_features, out_features, ia3_lr, is_feedforward=False):\n",
    "        nn.Module.__init__(self)\n",
    "        self.existing_layer = existing_layer\n",
    "        self.is_feedforward = is_feedforward\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.ia3_lw = (\n",
    "            nn.Parameter(\n",
    "                torch.ones((1, out_features), device=\"cuda\")\n",
    "            )\n",
    "            if not is_feedforward\n",
    "            else nn.Parameter(\n",
    "                torch.ones((1, in_features), device=\"cuda\")\n",
    "            )\n",
    "        )\n",
    "        nn.init.ones_(self.ia3_lw)\n",
    "\n",
    "        self.ia3_lr = ia3_lr\n",
    "\n",
    "\n",
    "    ## Everytime we run the forward method, we will get the corresponding weights and biases\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return F.linear(\n",
    "                x,\n",
    "                self.get_equivalent_weight(),\n",
    "                self.get_equivalent_bias()\n",
    "        )\n",
    "\n",
    "    def get_equivalent_weight(self):\n",
    "        \"\"\"\n",
    "        Converts IA3 layer to equivalent nn.Linear weight tensor\n",
    "        \"\"\"\n",
    "        mat = self.get_weight()\n",
    "        ret_weight = None\n",
    "        if not self.is_feedforward:\n",
    "            ret_weight = torch.diag(self.ia3_lw.view(-1)) @ mat\n",
    "        else:\n",
    "            ret_weight = mat @ torch.diag(self.ia3_lw.view(-1))\n",
    "\n",
    "        return ret_weight\n",
    "\n",
    "\n",
    "    def get_equivalent_bias(self):\n",
    "        \"\"\"\n",
    "        Gets equivalent nn.Linear bias data\n",
    "        \"\"\"\n",
    "        mat = self.get_bias()\n",
    "        if mat is None:\n",
    "            return None\n",
    "        ret_bias = None\n",
    "\n",
    "        if not self.is_feedforward:\n",
    "            ret_bias = torch.mul(mat, self.ia3_lw.squeeze())\n",
    "        else:\n",
    "            ret_bias = mat\n",
    "\n",
    "        return ret_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 2507278336\n",
      "Trainable Parameters: 1105920\n"
     ]
    }
   ],
   "source": [
    "inject_adapter(model, [\"k_proj\",\"v_proj\",\"down_proj\"], lambda x: IA3Adapter2(x, in_features=x.in_features, out_features=x.out_features, ia3_lr=1e-3, is_feedforward = [\"False\", \"False\", \"True\"]))\n",
    "total_params, trainable_params = ia3_params(model)\n",
    "\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Trainable Parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): IA3Adapter2(\n",
       "            (existing_layer): IA3Adapter2(\n",
       "              (existing_layer): IA3Adapter1(\n",
       "                (existing_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (v_proj): IA3Adapter2(\n",
       "            (existing_layer): IA3Adapter2(\n",
       "              (existing_layer): IA3Adapter1(\n",
       "                (existing_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): IA3Adapter2(\n",
       "            (existing_layer): IA3Adapter2(\n",
       "              (existing_layer): IA3Adapter1(\n",
       "                (existing_layer): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
