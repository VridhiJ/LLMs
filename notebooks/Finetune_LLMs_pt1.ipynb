{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color:green;font-size: 3em;\">\n",
    "Implementing Fine-tuning Techniques</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Implementing various fine-tuning methods as described in different papers, specifically LoRA and IA3.\n",
    "\n",
    "We will do this in 3 parts:\n",
    "\n",
    "Pt1:\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "- Evaluate the perplexity of a causal language model.\n",
    "- Fine-tune a sequence classification model using different learning rates and analyze its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets hf_xet -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, T5Tokenizer, T5ForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main types of models here: causal models and sequence classification models. The primary difference between them lies in their applications and functionality.\n",
    "\n",
    "#### Causal Models\n",
    "\n",
    "Causal models, also known as autoregressive models, generate the next word in a sequence based on the preceding words. They are used for tasks such as text generation, language modeling, machine translation, and speech recognition. These models operate unidirectionally, predicting the next token using only previous tokens.\n",
    "\n",
    "\n",
    "#### Sequence Classification Models\n",
    "\n",
    "Sequence classification models categorize a given input sequence into predefined categories. They are useful for tasks like sentiment analysis, spam detection, topic classification, named entity recognition (NER), and document classification. These models often process the entire input sequence at once, using context from all tokens to make a classification decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will initialize a causal model, specifically OPT-125m. When initializing a model, it is important to also initialize the corresponding tokenizer, as it handles the preprocessing of text data into a format that the model can understand.\n",
    "\n",
    "More about the OPT-125m model and its capabilities [here](https://huggingface.co/facebook/opt-125m).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a197d6c204e48879c4ebbfe06085552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f614940fbdfe448fa9aec2c2484269c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c125d3816741838486f6c4225ff985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1587468ede87475e9ded6bc0625eb98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69de0e9838dd48d78c45ae303a43878a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1e1d7a24524614bc1e25b76f5234b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fac57351c2b41e08724656c5f864eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8a25b0fc08474aae671cd5889e22bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the causal model\n",
    "causal_model_name = \"facebook/opt-125m\"\n",
    "causal_model = AutoModelForCausalLM.from_pretrained(causal_model_name).to(device)\n",
    "\n",
    "# Import the tokenizer\n",
    "causal_tokenizer = AutoTokenizer.from_pretrained(causal_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset for this task is Wikitext, which is a collection of articles from Wikipedia. This dataset is widely used for language modeling and text generation tasks because of its comprehensive and diverse range of topics. To read more about the Wikitext dataset and its features [here](https://huggingface.co/datasets/Salesforce/wikitext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d940dde6f5374adcb8bef184b6e97769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad72e0b540134343866f32aa72badb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525ecb72f5404460989702f0f5d03da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd2ac3602a3449782fdfac460497323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6083cbeb90ac4d1a9c47d57f0c48790f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9864293bfcbf42a198a288f02a55b3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0191adf2c4b34753bf68c26a2b6c2586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import wikitext dataset\n",
    "causal_test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "causal_test_encodings = causal_tokenizer(\"\\n\\n\".join(causal_test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will evaluate our model's effectiveness on the Wikitext dataset, an industry-standard benchmark for language modeling tasks. We will use perplexity to assess how well our model generates text.\n",
    "\n",
    "Perplexity measures how well a language model predicts the next word in a sequence. It is calculated as the exponentiated average negative log-likelihood of a sequence. A lower perplexity score indicates better performance, meaning the model is more accurate and confident in its predictions. Perplexity is a standard metric for comparing models and evaluating their ability to generate natural-sounding text. For more details on perplexity, read [here](https://huggingface.co/docs/transformers/en/perplexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the method to calculate the perplexity\n",
    "def calc_perplexity(model, encodings, stride):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "  model: our pretrained language model that we are evaluating on\n",
    "  encodings: input encodings containing input_ids and other relevant attributes\n",
    "  stride: the step size for segmenting the input sequence\n",
    "\n",
    "  Returns:\n",
    "  The perplexity of the model on the given dataset.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define max_length and seq_len\n",
    "  max_length = 1024\n",
    "  seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "  nlls = []\n",
    "  prev_end_loc = 0\n",
    "\n",
    "  # Loop through the sequence with the given stride\n",
    "  for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "      end_loc = min(begin_loc + max_length, seq_len)\n",
    "      trg_len = end_loc - prev_end_loc\n",
    "\n",
    "      # Get the input_ids for the current chunk and move to the correct device\n",
    "      input_ids = encodings.input_ids[:, begin_loc: end_loc].to(model.device)\n",
    "      target_ids = input_ids.clone()\n",
    "\n",
    "      # Mask out non-target positions\n",
    "      target_ids[:, :-trg_len] = -100\n",
    "\n",
    "      # Ensure no gradients are calculated\n",
    "      with torch.no_grad():\n",
    "          # Get the model outputs\n",
    "          outputs = model(input_ids, labels=target_ids)\n",
    "          neg_log_likelihood = outputs.loss\n",
    "\n",
    "      nlls.append(neg_log_likelihood)\n",
    "\n",
    "      prev_end_loc = end_loc\n",
    "      if end_loc == seq_len:\n",
    "          break\n",
    "\n",
    "  # Return the perplexity\n",
    "  return torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1120/1124 [04:21<00:00,  4.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(25.3988, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculate the perplexity of our causal model\n",
    "calc_perplexity(causal_model, causal_test_encodings, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Classification Model\n",
    "\n",
    "For sequence classification tasks, we will use T5-small, a model developed by Google and one of the more popular options available on Hugging Face. T5-small is a member of the T5 (Text-to-Text Transfer Transformer) family, which includes other models like T5-base and T5-large. While T5-small is efficient and suitable for many tasks, the larger models in this family offer more capacity and may provide improved performance but require significantly more computational resources and time for training and inference. Read more about T5-small [here](https://huggingface.co/google-t5/t5-small).\n",
    "\n",
    "\n",
    "\n",
    "First, we will import the sequence model that we are using and call it `seq_model`. Next, we will import the for the model and call it `seq_tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c38e5b89c6408bb7ed032d40f51b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498c639f51894851bd44780115d8a7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5afc9ca3f154dc59fc73d7d854aaef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f6c735f107462d8e3d9288f4d7e094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b23d7e3416d48c68815b19dd8961d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the sequence model\n",
    "seq_model_name = \"t5-small\"\n",
    "num_classes = 3\n",
    "seq_model = T5ForSequenceClassification.from_pretrained(seq_model_name, num_labels=num_classes).to(device)\n",
    "\n",
    "# Import the tokenizer\n",
    "seq_tokenizer = AutoTokenizer.from_pretrained(seq_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will be using is called CommitmentBank (CB). Each data point in this dataset consists of a premise, a hypothesis, and a label. The premise and hypothesis are both sentences, while the label is an integer from 0 to 2, indicating the relationship between the hypothesis and the premise in one of three categories: entailment, contradiction, or neutral.\n",
    "\n",
    "A key difference between the CB dataset and the WikiText dataset is that the CB dataset requires manual cleaning and preprocessing. This means we need to ensure the data is properly formatted and any noise is removed before it can be used for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c68cfcb1b9b49d1ac45e5bdf6950981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/18.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125cb9249eac479caee14a30785eea44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "super_glue.py:   0%|          | 0.00/30.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c24b7a540a4a7ba1dd44d9df3e7f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/75.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbb6fa3bb0947e0ba0d3cdc7aec206e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba25f63cf7c14d74807228d3ce74f8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a7e8ed42584ce9bee33c8f4e9c8a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the cb dataset\n",
    "# Input \"y\" for \"Do you wish to run the custom code?\"\n",
    "cb_dataset = load_dataset('super_glue', 'cb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['premise', 'hypothesis', 'idx', 'label']\n"
     ]
    }
   ],
   "source": [
    "print(cb_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Put sentence1 and sentence2 in a tuple\n",
    "    inputs = (examples[\"premise\"], examples[\"hypothesis\"])\n",
    "\n",
    "    # Put the inputs inside a tokenizer\n",
    "    result = seq_tokenizer(inputs[0],\n",
    "                           inputs[1],\n",
    "                           max_length=512,\n",
    "                           truncation=True,\n",
    "                           padding=\"max_length\",\n",
    "                           return_tensors=\"pt\",\n",
    "                           add_special_tokens=True)\n",
    "\n",
    "    result['labels'] = examples[\"label\"]\n",
    "\n",
    "    ## Delete unnecessary keys\n",
    "    del examples[\"premise\"]\n",
    "    del examples[\"hypothesis\"]\n",
    "    del examples[\"idx\"]\n",
    "    del examples[\"label\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd83b2cd14340108d4c118b2a11b82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 250 number of training examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e7a6c204e24af88472e90bef926888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 56 number of validation examples\n"
     ]
    }
   ],
   "source": [
    "# Load cb's train and validation sets\n",
    "seq_train_dataset = cb_dataset['train'].map(preprocess_function, batched=True)\n",
    "seq_train_loader = DataLoader(seq_train_dataset, batch_size=8)\n",
    "print(f\"There are {len(seq_train_dataset)} number of training examples\")\n",
    "\n",
    "seq_test_dataset = cb_dataset['validation'].map(preprocess_function, batched=True)\n",
    "seq_test_loader = DataLoader(seq_test_dataset, batch_size=8)\n",
    "print(f\"There are {len(seq_test_dataset)} number of validation examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sequence classification models, we don't usually use perplexity to calculate our performance. Instead, we calculate the accuracy of the model which is just the number of correct predictions / the number of total predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for batch in seq_test_loader:\n",
    "    print(batch.keys())\n",
    "    print(torch.stack(batch['input_ids'], dim = 0).shape)\n",
    "    print(torch.stack(batch['attention_mask'], dim = 0).shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in seq_test_loader:\n",
    "    labels = batch['labels']\n",
    "print(labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, labels):\n",
    "    preds = torch.argmax(predictions, dim=1).squeeze()\n",
    "    labels = labels.squeeze()\n",
    "\n",
    "    return (preds == labels).float().mean().item()\n",
    "\n",
    "\n",
    "def calc_accuracy(model, dataloader, device):\n",
    "  model.to(device)\n",
    "  model.eval()  # Set the model to evaluation mode\n",
    "  total_accuracy = 0\n",
    "  total_batches = 0\n",
    "\n",
    "\n",
    "  with torch.no_grad():  # Disable gradient calculation for inference\n",
    "      for batch in dataloader:\n",
    "        # Extract tensors from the batch dictionary\n",
    "        input_ids = batch['input_ids']  # List of tensors\n",
    "        attention_mask = batch['attention_mask']  # List of tensors\n",
    "        labels = batch['labels']  # Tensor of labels\n",
    "\n",
    "        input_ids = torch.stack(input_ids, dim = 0).transpose(0,1).to(device) # Shape [batch_size, seq_len]\n",
    "        attention_mask = torch.stack(attention_mask, dim = 0).transpose(0,1).to(device) # Shape [batch_size, seq_len]\n",
    "        labels = labels.to(device) # Shape [batch_size]\n",
    "\n",
    "\n",
    "        # Perform a forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "\n",
    "        # Compute accuracy for the current batch\n",
    "        batch_accuracy = compute_accuracy(predictions, labels)\n",
    "        total_accuracy += batch_accuracy\n",
    "        total_batches += 1\n",
    "\n",
    "  # Return the overall accuracy\n",
    "  return total_accuracy / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_accuracy = calc_accuracy(seq_model, seq_test_loader, device)\n",
    "print(f\"Final accuracy is {final_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
