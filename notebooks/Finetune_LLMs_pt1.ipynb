{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"color:green;font-size: 3em;\">\n",
    "Implementing Fine-tuning Techniques</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Implementing various fine-tuning methods as described in different papers, specifically LoRA and IA3.\n",
    "\n",
    "We will do this in 3 parts:\n",
    "\n",
    "Pt1:\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "- Evaluate the perplexity of a causal language model.\n",
    "- Fine-tune a sequence classification model using different learning rates and analyze its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets hf_xet -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, T5Tokenizer, T5ForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main types of models here: causal models and sequence classification models. The primary difference between them lies in their applications and functionality.\n",
    "\n",
    "#### Causal Models\n",
    "\n",
    "Causal models, also known as autoregressive models, generate the next word in a sequence based on the preceding words. They are used for tasks such as text generation, language modeling, machine translation, and speech recognition. These models operate unidirectionally, predicting the next token using only previous tokens.\n",
    "\n",
    "\n",
    "#### Sequence Classification Models\n",
    "\n",
    "Sequence classification models categorize a given input sequence into predefined categories. They are useful for tasks like sentiment analysis, spam detection, topic classification, named entity recognition (NER), and document classification. These models often process the entire input sequence at once, using context from all tokens to make a classification decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will initialize a causal model, specifically OPT-125m. When initializing a model, it is important to also initialize the corresponding tokenizer, as it handles the preprocessing of text data into a format that the model can understand.\n",
    "\n",
    "More about the OPT-125m model and its capabilities [here](https://huggingface.co/facebook/opt-125m).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the causal model\n",
    "causal_model_name = \"facebook/opt-125m\"\n",
    "causal_model = AutoModelForCausalLM.from_pretrained(causal_model_name).to(device)\n",
    "\n",
    "# Import the tokenizer\n",
    "causal_tokenizer = AutoTokenizer.from_pretrained(causal_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset for this task is Wikitext, which is a collection of articles from Wikipedia. This dataset is widely used for language modeling and text generation tasks because of its comprehensive and diverse range of topics. To read more about the Wikitext dataset and its features [here](https://huggingface.co/datasets/Salesforce/wikitext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import wikitext dataset\n",
    "causal_test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "causal_test_encodings = causal_tokenizer(\"\\n\\n\".join(causal_test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will evaluate our model's effectiveness on the Wikitext dataset, an industry-standard benchmark for language modeling tasks. We will use perplexity to assess how well our model generates text.\n",
    "\n",
    "Perplexity measures how well a language model predicts the next word in a sequence. It is calculated as the exponentiated average negative log-likelihood of a sequence. A lower perplexity score indicates better performance, meaning the model is more accurate and confident in its predictions. Perplexity is a standard metric for comparing models and evaluating their ability to generate natural-sounding text. For more details on perplexity, read [here](https://huggingface.co/docs/transformers/en/perplexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the method to calculate the perplexity\n",
    "def calc_perplexity(model, encodings, stride):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "  model: our pretrained language model that we are evaluating on\n",
    "  encodings: input encodings containing input_ids and other relevant attributes\n",
    "  stride: the step size for segmenting the input sequence\n",
    "\n",
    "  Returns:\n",
    "  The perplexity of the model on the given dataset.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define max_length and seq_len\n",
    "  max_length = 1024\n",
    "  seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "  nlls = []\n",
    "  prev_end_loc = 0\n",
    "\n",
    "  # Loop through the sequence with the given stride\n",
    "  for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "      end_loc = min(begin_loc + max_length, seq_len)\n",
    "      trg_len = end_loc - prev_end_loc\n",
    "\n",
    "      # Get the input_ids for the current chunk and move to the correct device\n",
    "      input_ids = encodings.input_ids[:, begin_loc: end_loc].to(model.device)\n",
    "      target_ids = input_ids.clone()\n",
    "\n",
    "      # Mask out non-target positions\n",
    "      target_ids[:, :-trg_len] = -100\n",
    "\n",
    "      # Ensure no gradients are calculated\n",
    "      with torch.no_grad():\n",
    "          # Get the model outputs\n",
    "          outputs = model(input_ids, labels=target_ids)\n",
    "          neg_log_likelihood = outputs.loss\n",
    "\n",
    "      nlls.append(neg_log_likelihood)\n",
    "\n",
    "      prev_end_loc = end_loc\n",
    "      if end_loc == seq_len:\n",
    "          break\n",
    "\n",
    "  # Return the perplexity\n",
    "  return torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1120/1124 [04:42<00:01,  3.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(25.3988, device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculate the perplexity of our causal model\n",
    "calc_perplexity(causal_model, causal_test_encodings, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Classification Model\n",
    "\n",
    "For sequence classification tasks, we will use T5-small, a model developed by Google and one of the more popular options available on Hugging Face. T5-small is a member of the T5 (Text-to-Text Transfer Transformer) family, which includes other models like T5-base and T5-large. While T5-small is efficient and suitable for many tasks, the larger models in this family offer more capacity and may provide improved performance but require significantly more computational resources and time for training and inference. Read more about T5-small [here](https://huggingface.co/google-t5/t5-small).\n",
    "\n",
    "\n",
    "\n",
    "First, we will import the sequence model that we are using and call it `seq_model`. Next, we will import the for the model and call it `seq_tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import the sequence model\n",
    "seq_model_name = \"t5-small\"\n",
    "num_classes = 3\n",
    "seq_model = T5ForSequenceClassification.from_pretrained(seq_model_name, num_labels=num_classes).to(device)\n",
    "\n",
    "# Import the tokenizer\n",
    "seq_tokenizer = AutoTokenizer.from_pretrained(seq_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will be using is called CommitmentBank (CB). Each data point in this dataset consists of a premise, a hypothesis, and a label. The premise and hypothesis are both sentences, while the label is an integer from 0 to 2, indicating the relationship between the hypothesis and the premise in one of three categories: entailment, contradiction, or neutral.\n",
    "\n",
    "A key difference between the CB dataset and the WikiText dataset is that the CB dataset requires manual cleaning and preprocessing. This means we need to ensure the data is properly formatted and any noise is removed before it can be used for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cb dataset\n",
    "# Input \"y\" for \"Do you wish to run the custom code?\"\n",
    "cb_dataset = load_dataset('super_glue', 'cb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['premise', 'hypothesis', 'idx', 'label']\n"
     ]
    }
   ],
   "source": [
    "print(cb_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Put sentence1 and sentence2 in a tuple\n",
    "    inputs = (examples[\"premise\"], examples[\"hypothesis\"])\n",
    "\n",
    "    # Put the inputs inside a tokenizer\n",
    "    result = seq_tokenizer(inputs[0],\n",
    "                           inputs[1],\n",
    "                           max_length=512,\n",
    "                           truncation=True,\n",
    "                           padding=\"max_length\",\n",
    "                           return_tensors=\"pt\",\n",
    "                           add_special_tokens=True)\n",
    "\n",
    "    result['labels'] = examples[\"label\"]\n",
    "\n",
    "    ## Delete unnecessary keys\n",
    "    del examples[\"premise\"]\n",
    "    del examples[\"hypothesis\"]\n",
    "    del examples[\"idx\"]\n",
    "    del examples[\"label\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 250 number of training examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd2326e125a4a618177fb8607c335e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 56 number of validation examples\n"
     ]
    }
   ],
   "source": [
    "# Load cb's train and validation sets\n",
    "seq_train_dataset = cb_dataset['train'].map(preprocess_function, batched=True)\n",
    "seq_train_loader = DataLoader(seq_train_dataset, batch_size=8)\n",
    "print(f\"There are {len(seq_train_dataset)} number of training examples\")\n",
    "\n",
    "seq_test_dataset = cb_dataset['validation'].map(preprocess_function, batched=True)\n",
    "seq_test_loader = DataLoader(seq_test_dataset, batch_size=8)\n",
    "print(f\"There are {len(seq_test_dataset)} number of validation examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sequence classification models, we don't usually use perplexity to calculate our performance. Instead, we calculate the accuracy of the model which is just the number of correct predictions / the number of total predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for batch in seq_test_loader:\n",
    "    print(batch.keys())\n",
    "    print(torch.stack(batch['input_ids'], dim = 0).shape)\n",
    "    print(torch.stack(batch['attention_mask'], dim = 0).shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in seq_test_loader:\n",
    "    labels = batch['labels']\n",
    "print(labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, labels):\n",
    "    preds = torch.argmax(predictions, dim=1).squeeze()\n",
    "    labels = labels.squeeze()\n",
    "\n",
    "    return (preds == labels).float().mean().item()\n",
    "\n",
    "\n",
    "def calc_accuracy(model, dataloader, device):\n",
    "  model.to(device)\n",
    "  model.eval()  # Set the model to evaluation mode\n",
    "  total_accuracy = 0\n",
    "  total_batches = 0\n",
    "\n",
    "\n",
    "  with torch.no_grad():  # Disable gradient calculation for inference\n",
    "      for batch in dataloader:\n",
    "        # Extract tensors from the batch dictionary\n",
    "        input_ids = batch['input_ids']  # List of tensors\n",
    "        attention_mask = batch['attention_mask']  # List of tensors\n",
    "        labels = batch['labels']  # Tensor of labels\n",
    "\n",
    "        input_ids = torch.stack(input_ids, dim = 0).transpose(0,1).to(device) # Shape [batch_size, seq_len]\n",
    "        attention_mask = torch.stack(attention_mask, dim = 0).transpose(0,1).to(device) # Shape [batch_size, seq_len]\n",
    "        labels = labels.to(device) # Shape [batch_size]\n",
    "\n",
    "\n",
    "        # Perform a forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "\n",
    "        # Compute accuracy for the current batch\n",
    "        batch_accuracy = compute_accuracy(predictions, labels)\n",
    "        total_accuracy += batch_accuracy\n",
    "        total_batches += 1\n",
    "\n",
    "  # Return the overall accuracy\n",
    "  return total_accuracy / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy is 0.10714285714285714\n"
     ]
    }
   ],
   "source": [
    "final_accuracy = calc_accuracy(seq_model, seq_test_loader, device)\n",
    "print(f\"Final accuracy is {final_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Fine-tuning\n",
    "\n",
    "In this section, we are going to fine-tune our entire sequence classification model. The purpose of fine-tuning is to adapt a pre-trained model, like T5 from Hugging Face, to our specific task of CB. By using the pre-existing knowledge encoded in the model's weights, fine-tuning allows us to improve performance on CB-related tasks by adjusting these weights based on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Parameters as Trainable\n",
    "\n",
    "First, we want to understand more about our model by making the parameters trainable and calculating the estimated GPU memory usage.\n",
    "\n",
    "\n",
    "- Set every parameter in our model to be trainable and caluclate the total number of trainable parameters as well as an estimated GPU memory usage of the trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set every parameter in the model as trainable\n",
    "def set_trainable_parameters(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of trainable parameters\n",
    "def num_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the estimated GPU memory usage in megabytes\n",
    "def estimated_gpu_memory_usage(num_params):\n",
    "    # 4 bytes of memory for each of the parameter; converted to MB\n",
    "    return (num_params * 4) / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  60770819\n",
      "GPU memory usage in MB 231.8222770690918\n"
     ]
    }
   ],
   "source": [
    "set_trainable_parameters(seq_model)\n",
    "num_params = num_trainable_parameters(seq_model)\n",
    "gpu_mem = estimated_gpu_memory_usage(num_params)\n",
    "print(\"Number of parameters: \", num_params)\n",
    "print(\"GPU memory usage in MB\", gpu_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Fine-tune Trainer Class\n",
    "\n",
    "Next, we want to show how different learning rates affect our fine-tuning. To do so, we will implement the methods inside our Trainer class so we can train and perform inference on our model.\n",
    "\n",
    "\n",
    "We need to implement the following methods inside the Trainer class:\n",
    "\n",
    "`__init__`\n",
    "\n",
    "- Initialize the model, learning rate, optimizer, device, and the loaders\n",
    "- Run the `set_seed` method provided below\n",
    "\n",
    "\n",
    "`train_one_epoch`\n",
    "\n",
    "- Trains the model on the train loader for one epoch and returns the loss and accuracy of that epoch\n",
    "\n",
    "`evaluate_one_epoch`\n",
    "\n",
    "-  Evaluates the modle on the validation loader for one epoch and returns the loss and accuracy of that epoch\n",
    "\n",
    "`train_and_evaluate`\n",
    "\n",
    "- Trains and evaluates the model on multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the trainer class\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, learning_rate=1e-5, seed=42):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.seed = seed\n",
    "        self.training_losses = [] # initializing the training loss\n",
    "        self.validation_losses = [] # initializing the validation loss\n",
    "        self.set_seed()\n",
    "\n",
    "    # Sets the seed\n",
    "    def set_seed(self):\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed_all(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        total_batches = 0\n",
    "\n",
    "        for batch in self.train_loader:\n",
    "            # Extract tensors from the batch dictionary\n",
    "            input_ids = batch['input_ids']  # List of tensors\n",
    "            attention_mask = batch['attention_mask']  # List of tensors\n",
    "            labels = batch['labels']  # Tensor of labels\n",
    "\n",
    "            input_ids = torch.stack(input_ids, dim = 0).transpose(0,1).to(device) # Shape [batch_size, seq_len]\n",
    "            attention_mask = torch.stack(attention_mask, dim = 0).transpose(0,1).to(device) # Shape [batch_size, seq_len]\n",
    "            labels = labels.to(device) # Shape [batch_size]\n",
    "\n",
    "            # Zero the gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()  # Update the model parameters\n",
    "\n",
    "            # Calculate accuracy\n",
    "            logits = outputs.logits\n",
    "            accuracy = compute_accuracy(logits, labels)\n",
    "            total_accuracy += accuracy\n",
    "            total_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        avg_accuracy = total_accuracy / total_batches\n",
    "        return avg_loss, avg_accuracy\n",
    "\n",
    "    def evaluate_one_epoch(self):\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        total_batches = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for batch in self.val_loader:\n",
    "                # Extract tensors from the batch dictionary\n",
    "                input_ids = batch['input_ids']  # List of tensors\n",
    "                attention_mask = batch['attention_mask']  # List of tensors\n",
    "                labels = batch['labels']  # Tensor of labels\n",
    "\n",
    "                input_ids = torch.stack(input_ids, dim = 0).transpose(0,1).to(device) # Shape [batch_size, seq_len]\n",
    "                attention_mask = torch.stack(attention_mask, dim = 0).transpose(0,1).to(device) # Shape [batch_size, seq_len]\n",
    "                labels = labels.to(device) # Shape [batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                logits = outputs.logits\n",
    "                accuracy = compute_accuracy(logits, labels)\n",
    "                total_accuracy += accuracy\n",
    "                total_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        avg_accuracy = total_accuracy / total_batches\n",
    "        return avg_loss, avg_accuracy\n",
    "\n",
    "    def train_and_evaluate(self, num_epochs, verbose=True):\n",
    "        for epoch in range(num_epochs):\n",
    "            # calculating training loss and accuracy for each epoch\n",
    "            train_loss, train_accuracy = self.train_one_epoch()\n",
    "            # calculating validation loss and accuracy for each epoch\n",
    "            val_loss, val_accuracy = self.evaluate_one_epoch()\n",
    "\n",
    "            self.training_losses.append(train_loss)\n",
    "            self.validation_losses.append(val_loss)\n",
    "\n",
    "            if verbose:\n",
    "                print(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "                print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "                print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n')\n",
    "\n",
    "        # Return a dictionary containing training and validation losses\n",
    "        return {'train_losses': self.training_losses, 'validation_losses': self.validation_losses}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the Trainer class, train the different learning rates on 5 epochs each (let the seed be 42) and record the train losses, validation losses, train accuracies, and validation accuracies for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training with learning rate: 0.1\n",
      "Epoch 1/5:\n",
      "Train Loss: 27.7169, Train Accuracy: 0.4688\n",
      "Validation Loss: 9.7497, Validation Accuracy: 0.4107\n",
      "\n",
      "Epoch 2/5:\n",
      "Train Loss: 32.6525, Train Accuracy: 0.5000\n",
      "Validation Loss: 12.6082, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 3/5:\n",
      "Train Loss: 39.1895, Train Accuracy: 0.6367\n",
      "Validation Loss: 37.0131, Validation Accuracy: 0.0893\n",
      "\n",
      "Epoch 4/5:\n",
      "Train Loss: 66.8707, Train Accuracy: 0.5820\n",
      "Validation Loss: 62.0436, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 5/5:\n",
      "Train Loss: 95.2911, Train Accuracy: 0.4922\n",
      "Validation Loss: 111.0555, Validation Accuracy: 0.5000\n",
      "\n",
      "\n",
      "\n",
      "Training with learning rate: 0.01\n",
      "Epoch 1/5:\n",
      "Train Loss: 71.9576, Train Accuracy: 0.3320\n",
      "Validation Loss: 12.1302, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 2/5:\n",
      "Train Loss: 11.2409, Train Accuracy: 0.4336\n",
      "Validation Loss: 11.8805, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 3/5:\n",
      "Train Loss: 9.1157, Train Accuracy: 0.4297\n",
      "Validation Loss: 10.4185, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 4/5:\n",
      "Train Loss: 8.6750, Train Accuracy: 0.4258\n",
      "Validation Loss: 9.5330, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 5/5:\n",
      "Train Loss: 8.2568, Train Accuracy: 0.5117\n",
      "Validation Loss: 10.1423, Validation Accuracy: 0.0893\n",
      "\n",
      "\n",
      "\n",
      "Training with learning rate: 0.001\n",
      "Epoch 1/5:\n",
      "Train Loss: 6.6841, Train Accuracy: 0.4375\n",
      "Validation Loss: 0.9440, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 2/5:\n",
      "Train Loss: 1.4062, Train Accuracy: 0.5430\n",
      "Validation Loss: 1.8895, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 3/5:\n",
      "Train Loss: 1.7094, Train Accuracy: 0.4062\n",
      "Validation Loss: 1.7445, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 4/5:\n",
      "Train Loss: 1.6227, Train Accuracy: 0.3828\n",
      "Validation Loss: 1.6647, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 5/5:\n",
      "Train Loss: 1.5806, Train Accuracy: 0.3828\n",
      "Validation Loss: 1.6493, Validation Accuracy: 0.5000\n",
      "\n",
      "\n",
      "\n",
      "Training with learning rate: 0.0001\n",
      "Epoch 1/5:\n",
      "Train Loss: 1.4070, Train Accuracy: 0.4648\n",
      "Validation Loss: 1.0019, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 2/5:\n",
      "Train Loss: 0.9938, Train Accuracy: 0.2852\n",
      "Validation Loss: 0.9308, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 3/5:\n",
      "Train Loss: 0.9246, Train Accuracy: 0.3555\n",
      "Validation Loss: 0.9325, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 4/5:\n",
      "Train Loss: 0.9225, Train Accuracy: 0.3203\n",
      "Validation Loss: 0.9329, Validation Accuracy: 0.5000\n",
      "\n",
      "Epoch 5/5:\n",
      "Train Loss: 0.9260, Train Accuracy: 0.2969\n",
      "Validation Loss: 0.9330, Validation Accuracy: 0.5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "epochs = 5\n",
    "seed = 42\n",
    "all_results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f'\\n\\nTraining with learning rate: {lr}')\n",
    "    trainer = Trainer(seq_model,\n",
    "                      seq_train_loader,\n",
    "                      seq_test_loader,\n",
    "                      learning_rate=lr,\n",
    "                      seed=seed)\n",
    "    all_results[lr] = trainer.train_and_evaluate(num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
