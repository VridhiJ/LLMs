{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNJGmLQVEMZ5uyBcO4hL5w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VridhiJ/Transformer-Architecture/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Architecture\n",
        "\n",
        "Implement the transformer architecture from scratch as outlined in the \"Attention is All You Need\" paper using PyTorch.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1qqIS189ikaXOpHUuwSMY67lCYyVTElAJ)"
      ],
      "metadata": {
        "id": "eItlNfn1mUVl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGgpcn6qk0qX",
        "outputId": "f67eec85-0490-46dc-f256-dd763a72e28d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Transformer-Architecture'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/VridhiJ/Transformer-Architecture.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Transformer-Architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mm0Z2aZlU0C",
        "outputId": "d8ee850f-8f04-44d9-d140-a84fe20a1190"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Transformer-Architecture\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math,copy,re\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "warnings.simplefilter(\"ignore\")\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmzADzkmmTh1",
        "outputId": "749d0911-d08e-49fd-9964-d6a48a3afcad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed value\n",
        "seed_value = 0\n",
        "\n",
        "# For CPU\n",
        "torch.manual_seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# For GPU (if using CUDA)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "FCyMg4ClB6oo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the \"Attention is All You Need\" paper, the authors used the following functions to create positional encoding. A cosine function is used for odd time steps, and a sine function is used for even time steps.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/524/1*yWGV9ck-0ltfV2wscUeo7Q.png\">\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/564/1*SgNlyFaHH8ljBbpCupDhSQ.png\">\n",
        "\n",
        "```\n",
        "pos -> refers to order in the sentence\n",
        "i -> refers to position along embedding vector dimension\n",
        "```\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/906/1*B-VR6R5vJl3Y7jbMNf5Fpw.png\">"
      ],
      "metadata": {
        "id": "tnyrbFfUCRaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,max_seq_len, model_dimension_embed):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      max_seq_len (int): length of the input sequence\n",
        "      model_dimension_embed (int): embedding dimension of the model\n",
        "    \"\"\"\n",
        "    super(PositionalEncoding,self).__init__()\n",
        "    self.embed_dim = model_dimension_embed\n",
        "\n",
        "    # Initialize the positional encoding matrix using the above equation\n",
        "    pe = torch.zeros(max_seq_len, self.embed_dim)\n",
        "    for pos in range(max_seq_len):\n",
        "        for i in range(self.embed_dim // 2):\n",
        "            pe[pos, 2*i] = math.sin(pos / (10000 ** (2*i / self.embed_dim)))\n",
        "            pe[pos, 2*i+1] = math.cos(pos / (10000 ** (2*i / self.embed_dim)))\n",
        "\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: input vector\n",
        "    Returns:\n",
        "      x: output vector with positional embedding added\n",
        "    \"\"\"\n",
        "    # Weight the embeddings relatively larger\n",
        "    x = x * math.sqrt(self.embed_dim)\n",
        "\n",
        "    # Add positional encoding to the input embeddings\n",
        "    seq_len = x.size(1)\n",
        "    x = x + torch.autograd.Variable(self.pe[:, :seq_len, :], requires_grad=False)\n",
        "    return x"
      ],
      "metadata": {
        "id": "0Li1j0JZn7NV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x_u1XlTQDOWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,embed_dim=512, n_heads=8):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      embed_dim: embedding dimension\n",
        "      n_heads: number of attention heads\n",
        "    \"\"\"\n",
        "    super(MultiHeadAttention,self).__init__()\n",
        "\n",
        "    self.embed_dim = embed_dim # 512 dim\n",
        "    self.n_heads = n_heads # 8 dim\n",
        "    self.single_head_dim = embed_dim // n_heads # 512 / 8 = 64, each key, query, and value head will be 64d\n",
        "\n",
        "    # Initialize key, query and value matrices\n",
        "    self.key_matrix = nn.Linear(self.embed_dim, self.embed_dim, bias = False)\n",
        "    self.query_matrix = nn.Linear(self.embed_dim, self.embed_dim, bias = False)\n",
        "    self.value_matrix = nn.Linear(self.embed_dim, self.embed_dim, bias = False)\n",
        "\n",
        "    # Initialize output projection matrix\n",
        "    self.out = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "  def forward(self, key, query, value, mask=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      key: key vector\n",
        "      query: query vector\n",
        "      value: value vector\n",
        "      mask: mask to be applied to the attention scores(for decoder)\n",
        "\n",
        "    Returns:\n",
        "      output: vector from multi-head attention\n",
        "    \"\"\"\n",
        "    batch_size = key.size(0)\n",
        "    seq_length = key.size(1)\n",
        "    seq_length_query = query.size(1)\n",
        "\n",
        "    # Apply linear transformation to the key, query and value matrices\n",
        "    k = self.key_matrix(key)\n",
        "    q = self.query_matrix(query)\n",
        "    v = self.value_matrix(value)\n",
        "\n",
        "    # Reshape key, query and value\n",
        "    k = k.view(batch_size, seq_length, self.n_heads, self.single_head_dim)\n",
        "    q = q.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim)\n",
        "    v = v.view(batch_size, seq_length, self.n_heads, self.single_head_dim)\n",
        "\n",
        "    # Transpose key, query and value\n",
        "    k = k.transpose(1,2)\n",
        "    q = q.transpose(1,2)\n",
        "    v = v.transpose(1,2)\n",
        "\n",
        "    # Compute attention score\n",
        "    k_adjusted = k.transpose(-1,-2)\n",
        "    product = torch.matmul(q, k_adjusted)\n",
        "\n",
        "    if mask is not None:\n",
        "      if len(mask.size()) == 4:\n",
        "        product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "      else:\n",
        "        mask = mask.unsqueeze(1) # Adds an extra dimension for the heads\n",
        "        product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "    product = product / math.sqrt(self.single_head_dim)\n",
        "    scores = F.softmax(product, dim=-1)\n",
        "\n",
        "    # Compute weighted sum of value vectors and run it through the last layer\n",
        "    scores = torch.matmul(scores, v)\n",
        "    concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim * self.n_heads)\n",
        "    output = self.out(concat)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "27UDjoCn1u7h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           embed_dim: dimension of the embedding\n",
        "           expansion_factor: factor determining output dimension of the linear layer\n",
        "           n_heads: number of attention heads\n",
        "        \"\"\"\n",
        "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, expansion_factor * embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(expansion_factor * embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, key, query, value):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           key: key vector\n",
        "           query: query vector\n",
        "           value: value vector\n",
        "\n",
        "        Returns:\n",
        "           norm2_out: output of transformer block\n",
        "        \"\"\"\n",
        "        # Calculate attention output using self.attention\n",
        "        attention_out = self.attention(key, query, value)  # 32x10x512\n",
        "\n",
        "        # Do dropout, add residual connection and normalize\n",
        "        intermediate = self.dropout1(attention_out)\n",
        "        norm1_out = self.norm1(intermediate + value)\n",
        "\n",
        "        # Pass through feed forward layer\n",
        "        feed_fwd_out = self.feed_forward(norm1_out)  # 32x10x512 -> 32x10x2048 -> 32x10x512\n",
        "\n",
        "        # Do dropout, add residual connection and normalize\n",
        "        intermediate2 = self.dropout2(feed_fwd_out)\n",
        "        norm2_out = self.norm2(intermediate2 + norm1_out) # 32x10x512\n",
        "\n",
        "        return norm2_out\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        seq_len : length of input sequence\n",
        "        embed_dim: dimension of embedding\n",
        "        num_layers: number of encoder layers\n",
        "        expansion_factor: factor determining the number of linear layers in feed-forward layer\n",
        "        n_heads: number of heads in multi-head attention\n",
        "\n",
        "    Returns:\n",
        "        out: output of the encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoder = PositionalEncoding(seq_len, embed_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, expansion_factor, n_heads) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the embedding layer\n",
        "        embed_out = self.embedding_layer(x)  # 32x10 -> 32x10x512\n",
        "\n",
        "        # Apply positional encoding\n",
        "        out = self.positional_encoder(embed_out)  # 32x10x512\n",
        "\n",
        "        # Pass through each TransformerBlock\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out)  # 32x10x512\n",
        "\n",
        "        # Return the final output\n",
        "        return out  # 32x10x512\n",
        "\n"
      ],
      "metadata": {
        "id": "PdDnOjoNnc3B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           embed_dim: dimension of the embedding\n",
        "           expansion_factor: factor determining output dimension of the linear layer\n",
        "           n_heads: number of attention heads\n",
        "        \"\"\"\n",
        "        self.attention = MultiHeadAttention(embed_dim, n_heads=8)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.transformer_block = TransformerBlock(embed_dim, expansion_factor, n_heads)\n",
        "\n",
        "    def forward(self, key, query, value, mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           key: key vector\n",
        "           query: query vector\n",
        "           value: value vector\n",
        "           mask: mask to be given for multi-head attention\n",
        "\n",
        "        Returns:\n",
        "           out: output of transformer block\n",
        "        \"\"\"\n",
        "        # Implement masked attention with the given mask\n",
        "        attention = self.attention(query, query, query, mask=mask)  # 32x10x512\n",
        "        # Do dropout, add residual connection, and then apply normalization\n",
        "        intermediate = self.dropout(attention)\n",
        "        query = self.norm(intermediate + query)\n",
        "\n",
        "        # Pass through the transformer block\n",
        "        out = self.transformer_block(key, query, value)\n",
        "\n",
        "        return out\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers=2, expansion_factor=4, n_heads=8):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           target_vocab_size: vocabulary size of the target\n",
        "           embed_dim: dimension of embedding\n",
        "           seq_len: length of input sequence\n",
        "           num_layers: number of decoder layers\n",
        "           expansion_factor: factor determining the number of linear layers in the feed-forward layer\n",
        "           n_heads: number of heads in multi-head attention\n",
        "        \"\"\"\n",
        "        self.word_embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
        "        self.position_embedding = PositionalEncoding(seq_len, embed_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [DecoderBlock(embed_dim, expansion_factor=4, n_heads=8) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, enc_out, mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input vector from target\n",
        "            enc_out: output from encoder layer\n",
        "            trg_mask: mask for decoder self-attention\n",
        "\n",
        "        Returns:\n",
        "            out: output vector\n",
        "        \"\"\"\n",
        "        x = self.word_embedding(x)  # 32x10x512\n",
        "\n",
        "        x = self.position_embedding(x)  # 32x10x512\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(enc_out, x, enc_out, mask)\n",
        "\n",
        "        out = F.softmax(self.fc_out(x), dim=-1)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "n3Kamg8Mnc0O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        src_vocab_size,\n",
        "        target_vocab_size,\n",
        "        seq_length,\n",
        "        num_labels,\n",
        "        num_layers=2,\n",
        "        expansion_factor=4,\n",
        "        n_heads=8\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           embed_dim: dimension of embedding\n",
        "           src_vocab_size: vocabulary size of source\n",
        "           target_vocab_size: vocabulary size of target\n",
        "           seq_length: length of input sequence\n",
        "           num_layers: number of encoder layers\n",
        "           expansion_factor: factor determining the number of linear layers in the feed-forward layer\n",
        "           n_heads: number of heads in multi-head attention\n",
        "        \"\"\"\n",
        "\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "\n",
        "        self.encoder = TransformerEncoder(\n",
        "             seq_length, src_vocab_size, embed_dim,\n",
        "            num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads\n",
        "        )\n",
        "        self.decoder = TransformerDecoder(\n",
        "            target_vocab_size, embed_dim, seq_length,\n",
        "            num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads\n",
        "        )\n",
        "        self.lm_head = nn.Linear(target_vocab_size, num_labels - 1)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            trg: target sequence\n",
        "\n",
        "        Returns:\n",
        "            trg_mask: target mask\n",
        "        \"\"\"\n",
        "        # TODO: Implement the mask for the target sequence\n",
        "        batch_size, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len), device=trg.device)).bool() # [trg_len, trg_len]\n",
        "        trg_mask = trg_mask.unsqueeze(0).expand(batch_size, -1, -1) # [batch_size, trg_len, trg_len]\n",
        "        return trg_mask\n",
        "\n",
        "    def decode(self, src, trg):\n",
        "        \"\"\"\n",
        "        Helper function for inference\n",
        "\n",
        "        Args:\n",
        "            src: input to encoder\n",
        "            trg: input to decoder\n",
        "\n",
        "        Returns:\n",
        "            out_labels: final prediction of sequence\n",
        "        \"\"\"\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_out = self.encoder(src)\n",
        "        out_labels = []\n",
        "        batch_size, seq_len = src.shape[0], src.shape[1]\n",
        "        out = trg\n",
        "        for i in range(seq_len):\n",
        "            out = self.decoder(out, enc_out, trg_mask)  # bs x seq_len x vocab_dim\n",
        "            out = out[:, -1, :].argmax(-1)\n",
        "            out_labels.append(out.item())\n",
        "            out = torch.unsqueeze(out, axis=0)\n",
        "        return out_labels\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: input to encoder\n",
        "            trg: input to decoder\n",
        "\n",
        "        Returns:\n",
        "            output: final vector which returns probabilities of each target word\n",
        "        \"\"\"\n",
        "        # Implement the forward function for the Transformer class\n",
        "\n",
        "        #Use src and trg to get the outputs from the encoder and decoder (make sure to pass in the mask where appropriate)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_out = self.encoder(src)\n",
        "        dec_out = self.decoder(trg, enc_out, trg_mask)\n",
        "\n",
        "        #Take the average across the sequence length of the decoder's output\n",
        "        output = dec_out.mean(dim = 1)  # Pooling across the sequence length to get [batch_size, d_model]\n",
        "\n",
        "        # Call the lm_head to get the predicted token for each batch\n",
        "        output = self.lm_head(output) # Shape: [batch_size, 1]\n",
        "\n",
        "        # Remove the extra dimension\n",
        "        return output.squeeze(-1)  # Shape: [batch_size]"
      ],
      "metadata": {
        "id": "0b8Hksgkncr5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " We will be training the model on a binary classification problem that we will generate here"
      ],
      "metadata": {
        "id": "OT01EWiG9MXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20\n",
        "num_layers = 6\n",
        "seq_length= 12\n",
        "\n",
        "\n",
        "  # let 0 be sos token and 1 be eos token\n",
        "\n",
        "model = Transformer(\n",
        "      embed_dim=512,\n",
        "      src_vocab_size=vocab_size,\n",
        "      target_vocab_size=vocab_size,\n",
        "      seq_length=seq_length,\n",
        "      num_labels=2,\n",
        "      num_layers=num_layers,\n",
        "      expansion_factor=4,\n",
        "      n_heads=8\n",
        "  )"
      ],
      "metadata": {
        "id": "hdlJJyV4udNi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_data(num_samples=1000, seq_length=10, vocab_size=100):\n",
        "    # Ensure middle values follow a normal distribution\n",
        "    middle_mean = vocab_size // 2\n",
        "    middle_std = vocab_size // 4\n",
        "    middle_values = torch.normal(middle_mean, middle_std, (num_samples, seq_length - 4))\n",
        "\n",
        "    # Clip values to be within the valid range\n",
        "    middle_values = torch.clamp(middle_values, min=10, max=vocab_size - 1).long()\n",
        "    other_values = torch.randint(1, vocab_size, (num_samples, 2))  # Changed to vocab_size\n",
        "\n",
        "    start = torch.zeros((num_samples, 1), dtype=torch.long)\n",
        "    end = torch.ones((num_samples, 1), dtype=torch.long)\n",
        "\n",
        "    data = torch.cat((start, other_values[:, :1], middle_values, other_values[:, 1:], end), dim=1)\n",
        "\n",
        "    # Create target sequences by shifting source sequences to the right\n",
        "    trg_data = torch.cat((start, data[:, :-1]), dim=1)\n",
        "\n",
        "    # Generate labels: for example, label is 1 if the sequence contains a value > 18 in the middle part\n",
        "    labels = (middle_values > 15).any(dim=1).long()\n",
        "\n",
        "    return data, trg_data, labels"
      ],
      "metadata": {
        "id": "XRxTP9bwuhfz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "hLvtdZzcuk7k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_src, train_trg, train_labels = generate_synthetic_data(vocab_size=vocab_size)\n",
        "test_src, test_trg, test_labels = generate_synthetic_data(vocab_size=vocab_size)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(train_src, train_trg, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_src, test_trg, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "cuEz6lTgunPR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.sum(train_labels))\n",
        "print(train_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bykr5FVupqe",
        "outputId": "c45a6237-02ee-4206-8483-fff859ba9306"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(547)\n",
            "torch.Size([1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for src, trg, labels in test_loader:\n",
        "      outputs = model(src, trg)  # Shape: [batch_size]\n",
        "      outputs = outputs.squeeze()  # Shape: [batch_size]\n",
        "      predictions = torch.round(torch.sigmoid(outputs))  # Shape: [batch_size]\n",
        "      total += labels.size(0)\n",
        "      correct += (predictions == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / (total)  # Multiply by sequence length to get the total number of elements\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us4Hpwt3urn_",
        "outputId": "2b4bfa20-c060-444d-f8d8-f1e1c1b3ea5d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.4920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=0.00010). Accuracy came out to be 0.508\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00020) #Accuracy 0.821"
      ],
      "metadata": {
        "id": "24AM5rrwuwsP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, trg, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(src, trg)\n",
        "        loss = criterion(outputs, labels.float())  # BCEWithLogitsLoss expects float labels\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_N7uAoLuzXU",
        "outputId": "f06c3fa9-533d-43e8-8718-0caa8acd0eb2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.6660\n",
            "Epoch 2/5, Loss: 0.6415\n",
            "Epoch 3/5, Loss: 0.6381\n",
            "Epoch 4/5, Loss: 0.6377\n",
            "Epoch 5/5, Loss: 0.6356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for src, trg, labels in test_loader:\n",
        "        outputs = model(src, trg)  # Shape: [batch_size]\n",
        "        outputs = outputs.squeeze()  # Shape: [batch_size]\n",
        "        predictions = torch.round(torch.sigmoid(outputs))  # Shape: [batch_size]\n",
        "        total += labels.size(0)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "\n",
        "    print(correct)\n",
        "    accuracy = correct / (total)  # Multiply by sequence length to get the total number of elements\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-TjzEIJu2MF",
        "outputId": "64ce6b77-6b92-4fca-acc8-2edb5229d243"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "821\n",
            "Test Accuracy: 0.8210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving Transformer Architecture with SwiGLU Activation"
      ],
      "metadata": {
        "id": "VA6NxbQIC61Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing ReLU (Rectified Linear Unit) activation function with SwiGLU (SwiGLU: Swish-Gated Linear Unit), which has been adopted in models like LLaMA.\n",
        "\n",
        "## Understanding Activation Functions\n",
        "\n",
        "Activation functions are crucial components in neural networks that introduce non-linearity into the model, enabling it to learn complex patterns. The ReLU function is a popular choice due to its simplicity and effectiveness. It is defined as:\n",
        "\n",
        "$ \\text{ReLU}(x) = \\max(0, x) $\n",
        "\n",
        "While ReLU has been successful, it has limitations such as the \"dying ReLU\" problem where neurons can become inactive and only output zero for any input.\n",
        "\n",
        "SwiGLU, introduced in 2020, is one such replacement activation function. The name SwiGLU is derived from two other activation functions: Swish and GLU. Let's explore these components in detail.\n",
        "<hr>\n",
        "\n",
        "<h3><strong>Swish Activation Function</strong></h3>\n",
        "\n",
        "Swish is a non-linear activation function defined as follows:\n",
        "\n",
        "$$ \\text{Swish}(x) = x \\cdot \\sigma(\\beta x) $$\n",
        "\n",
        "where $\\sigma$ represents the sigmoid function and $\\beta$ is a learnable parameter. Swish can outperform the ReLU activation function because it provides a smoother transition around 0, potentially leading to better optimization.\n",
        "\n",
        "<h3><strong>Gated Linear Unit (GLU)</strong></h3>\n",
        "\n",
        "Gated Linear Units (GLUs) are neural network layers defined as the component-wise product of two linear transformations, one of which is activated by a sigmoid function. The GLU can be represented by the following equation:\n",
        "\n",
        "$$ \\text{GLU}(x) = \\sigma(W_1 x + b) \\otimes (V x + c) $$\n",
        "\n",
        "GLUs have proven effective in capturing long-range dependencies in sequences, addressing some of the vanishing gradient problems associated with other gating mechanisms like those in LSTMs and GRUs.\n",
        "\n",
        "<h3><strong>SwiGLU</strong></h3>\n",
        "\n",
        "SwiGLU combines the concepts of Swish and GLU. Instead of using a sigmoid activation function, SwiGLU uses Swish with $\\beta = 1$. The resulting formula for SwiGLU is:\n",
        "\n",
        "$$ \\text{SwiGLU}(x) = \\text{Swish}(W_1 x + b) \\otimes (V x + c) $$\n",
        "\n"
      ],
      "metadata": {
        "id": "cOgIs-3mC1ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement forward function of the SwiGLU class\n",
        "class SwiGLU(nn.Module):\n",
        "\n",
        "    def __init__(self, w1, w2, w3) -> None:\n",
        "        super().__init__()\n",
        "        self.w1 = w1\n",
        "        self.w2 = w2\n",
        "        self.w3 = w3\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implement forward function of SwiGLU\n",
        "        x1 = F.linear(x, self.w1.weight.T)\n",
        "        x2 = F.linear(x, self.w2.weight)\n",
        "        hidden = F.silu(x1) * x2\n",
        "\n",
        "        return F.linear(hidden, self.w3.weight)"
      ],
      "metadata": {
        "id": "vGH-p83tDCvF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrate SwiGLU and Compare Performance\n",
        "\n",
        "1. **Update the TransformerBlock:** Replace the ReLU activation function in the `TransformerBlock` with the `SwiGLU` function.\n",
        "\n",
        "2. **Update the Main Transformer Class:** Ensure that all components of the `Transformer` class are using the updated `TransformerBlock` with `SwiGLU`.\n",
        "\n",
        "3. **Train and Evaluate:** Train your updated Transformer model on the provided dataset and evaluate its performance. Compare the performance of the Transformer model using SwiGLU with the original Transformer model using ReLU."
      ],
      "metadata": {
        "id": "bgfohTpCDnBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           embed_dim: dimension of the embedding\n",
        "           expansion_factor: factor determining output dimension of the linear layer\n",
        "           n_heads: number of attention heads\n",
        "        \"\"\"\n",
        "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        hidden_dim = 4 * embed_dim # 4x512\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        hidden_dim = expansion_factor * ((hidden_dim + expansion_factor - 1) // expansion_factor)\n",
        "\n",
        "        self.swi_glu = SwiGLU(\n",
        "            nn.Linear(expansion_factor*embed_dim, hidden_dim, bias=False), # 2048x2048\n",
        "            nn.Linear(hidden_dim, expansion_factor*embed_dim, bias=False), #2048x2048\n",
        "            nn.Linear(expansion_factor*embed_dim, hidden_dim, bias=False) #2048x2048\n",
        "        )\n",
        "\n",
        "        # TODO: Use swiglu along with linear layers\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim), #2048\n",
        "            #nn.ReLU(),\n",
        "            self.swi_glu,\n",
        "            nn.Linear(hidden_dim, embed_dim) #512\n",
        "\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, key, query, value):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           key: key vector\n",
        "           query: query vector\n",
        "           value: value vector\n",
        "\n",
        "        Returns:\n",
        "           norm2_out: output of transformer block\n",
        "        \"\"\"\n",
        "        attention_out = self.attention(key, query, value)  # 32x10x512\n",
        "\n",
        "        # TODO: Do something similar as the encoder in the previous homework\n",
        "        intermediate = self.dropout1(attention_out)\n",
        "        norm1_out = self.norm1(intermediate + value)\n",
        "\n",
        "        # Use SwiGLU in feed-forward layer\n",
        "        feed_fwd_out = self.feed_forward(norm1_out)\n",
        "\n",
        "        intermediate2 = self.dropout2(feed_fwd_out)\n",
        "        norm2_out = self.norm2(intermediate2 + norm1_out)\n",
        "\n",
        "        return norm2_out\n"
      ],
      "metadata": {
        "id": "dew2TGgfDjay"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20\n",
        "num_layers = 6\n",
        "seq_length= 12\n",
        "\n",
        "\n",
        "# let 0 be sos token and 1 be eos token\n",
        "\n",
        "model2 = Transformer(\n",
        "    embed_dim=512,\n",
        "    src_vocab_size=vocab_size,\n",
        "    target_vocab_size=vocab_size,\n",
        "    seq_length=seq_length,\n",
        "    num_labels=2,\n",
        "    num_layers=num_layers,\n",
        "    expansion_factor=4,\n",
        "    n_heads=8\n",
        ")"
      ],
      "metadata": {
        "id": "4C--g_5TDjYD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.00020) # Accuracy 0.817"
      ],
      "metadata": {
        "id": "241wvya5DjUk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model2.train()\n",
        "    epoch_loss = 0\n",
        "    for src, trg, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model2(src, trg)\n",
        "        loss = criterion(outputs, labels.float())  # BCEWithLogitsLoss expects float labels\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BACGQMHEFSP",
        "outputId": "febd5116-ffc7-4511-eee7-e018813916b2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.6675\n",
            "Epoch 2/5, Loss: 0.6325\n",
            "Epoch 3/5, Loss: 0.6314\n",
            "Epoch 4/5, Loss: 0.6334\n",
            "Epoch 5/5, Loss: 0.6316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for src, trg, labels in test_loader:\n",
        "        outputs = model2(src, trg)  # Shape: [batch_size]\n",
        "        outputs = outputs.squeeze()  # Shape: [batch_size]\n",
        "        predictions = torch.round(torch.sigmoid(outputs))  # Shape: [batch_size]\n",
        "        total += labels.size(0)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "\n",
        "    print(correct)\n",
        "    accuracy = correct / (total)  # Multiply by sequence length to get the total number of elements\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_AvtqlFEFPX",
        "outputId": "5220c798-a554-41bb-df5e-880e2c2df33e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "817\n",
            "Test Accuracy: 0.8170\n"
          ]
        }
      ]
    }
  ]
}
